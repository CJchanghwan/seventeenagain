{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"최종검증.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 라이브러리 호출"],"metadata":{"id":"Q2PCv81nePui"}},{"cell_type":"code","source":["pip install segmentation-models-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37PXMf-st-_Q","executionInfo":{"status":"ok","timestamp":1653373498537,"user_tz":-540,"elapsed":8327,"user":{"displayName":"ᄋᄉᄋ","userId":"01562750966594764538"}},"outputId":"fbf0b4b2-5085-46fc-bc5c-dd4693bf9476"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.2.1-py3-none-any.whl (88 kB)\n","\u001b[K     |████████████████████████████████| 88 kB 3.5 MB/s \n","\u001b[?25hCollecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 35.6 MB/s \n","\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n","  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n","Collecting pretrainedmodels==0.7.4\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.12.0+cu113)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.11.0+cu113)\n","Collecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.0.4)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=d7f86a15246ef855e601a45b838eba9ef76e4cfa59e14ab40c7018877083ad09\n","  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=217ce786fae463be5109dc27399113a20641c4584fc8fe34dbcc2de4188a4e01\n","  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.1 timm-0.4.12\n"]}]},{"cell_type":"code","source":["pip install pytorch-msssim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8qv0dzhzva3e","executionInfo":{"status":"ok","timestamp":1653373582727,"user_tz":-540,"elapsed":3539,"user":{"displayName":"ᄋᄉᄋ","userId":"01562750966594764538"}},"outputId":"72d7bdda-6329-4084-fb1b-6db4de1ca5e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-msssim\n","  Downloading pytorch_msssim-0.2.1-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-msssim) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-msssim) (4.2.0)\n","Installing collected packages: pytorch-msssim\n","Successfully installed pytorch-msssim-0.2.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZPOx2M5bwIH"},"outputs":[],"source":[" import os, tqdm, math, random, argparse\n","\n","import numpy as np\n","import pandas as pd\n","\n","from glob import glob\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF\n","from torch.utils.data import DataLoader, Dataset\n","\n","from tqdm.notebook import tqdm\n","\n","import segmentation_models_pytorch as smp\n","from pytorch_msssim import MS_SSIM\n","\n","import cv2\n","import random"]},{"cell_type":"code","source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(device)\n","print(device)\n","\n","#device는 학습 환경을 cpu로 할 것인지 아니면 gpu로 할것인지 에 대한 변수이다.\n","# torch.cuda.is_available()는 말 그대로 cuda(gpu)을 사용할 수 있으면 사용하겠다는 의미"],"metadata":{"id":"DR7e0y8obzvl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653373609521,"user_tz":-540,"elapsed":284,"user":{"displayName":"ᄋᄉᄋ","userId":"01562750966594764538"}},"outputId":"e51cece3-4c18-40db-ac54-81f03ce454cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["# directory 확인"],"metadata":{"id":"djLIIthHeXAt"}},{"cell_type":"code","source":["train_csv = pd.read_csv('/content/drive/MyDrive/train.csv')\n","test_csv = pd.read_csv('/content/drive/MyDrive/test.csv')"],"metadata":{"id":"rwMEodnncEa7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_csv.head()"],"metadata":{"id":"czJgJvlScHd4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_csv.head()"],"metadata":{"id":"n-SaZVDPcJPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_all_input_files = '/content/drive/MyDrive/train/'+train_csv['input_img']\n","train_all_label_files = '/content/drive/MyDrive/label/'+train_csv['label_img']\n","test_all_input_files = '/content/drive/MyDrive/test/'+test_csv['input_img']"],"metadata":{"id":"rhdt4yBtcK_I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 학습 데이터와 검증 데이터를 분리"],"metadata":{"id":"m9JQ1fKTeiQ-"}},{"cell_type":"code","source":["train_input_files = train_all_input_files[60:].to_numpy()\n","train_label_files = train_all_label_files[60:].to_numpy()"],"metadata":{"id":"41gFQ0yXcNXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vaild_input_files = train_all_input_files[:60].to_numpy()\n","vaild_label_files = train_all_label_files[:60].to_numpy()"],"metadata":{"id":"v5w7DZukcPbL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input_files"],"metadata":{"id":"cP2_MGo2cRkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vaild_input_files"],"metadata":{"id":"2Dex_b8qcTTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 빛번짐 데이터 열어서 확인"],"metadata":{"id":"-zLl24_feoZd"}},{"cell_type":"code","source":["for i in range(4):\n","    origin_img = Image.open(f'/content/drive/MyDrive/train/train_input_{i+10060}.png')\n","    trans_img = Image.open(f'/content/drive/MyDrive/label/train_label_{i+10060}.png')\n","    fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n","    ax[0].set_title('input')\n","    ax[0].imshow(origin_img)\n","    ax[1].set_title('label')\n","    ax[1].imshow(trans_img)\n","    plt.show()"],"metadata":{"id":"d3UJKcXWcWSb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 빛번짐 최소화 데이터 처리"],"metadata":{"id":"znRaShqlfEIg"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, data, label,w,h ,is_train=True):\n","        self.data  = data\n","        self.label = label\n","        self.w = w\n","        self.h = h\n","        self.is_train = is_train\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def transform(self, image, label):\n","        \n","        resizer = transforms.Resize(size=(self.h, self.w))\n","        image = resizer(image)\n","        label = resizer(label)\n","\n","        image = TF.to_tensor(image)\n","        label = TF.to_tensor(label)\n","        return image, label\n","    \n","    def __getitem__(self, idx):\n","        origin_img = Image.open(self.data[idx])\n","        label_img  = Image.open(self.label[idx])\n","        \n","        origin_img, label_img = self.transform(origin_img, label_img)\n","        return origin_img, label_img\n","    \n","#def __init__는 생성자\n","#def __len__은 데이셋의 길이를 변환\n","#def __transform 에서 resize = 이미지 크기 재지정 , to_tensor = 이미지를 tensor형으로 변경\n","#def __getitem__은 이미지를 호출하고 transform함수를 적용 시킨 후 이미지를 호출"],"metadata":{"id":"5DLEquF9caAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class testDataset(Dataset):\n","    def __init__(self, data,is_train=True):\n","        self.data  = data\n","        self.is_train = is_train\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def transform(self, image):\n","        \n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Resize(size=(384, 512))\n","        ])\n","        image = transform(image)\n","        \n","        return image\n","    \n","    def __getitem__(self, idx):\n","        origin_img = Image.open(self.data[idx])\n","        \n","        origin_img = self.transform(origin_img)\n","        return origin_img"],"metadata":{"id":"psNGGVO0cdTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = CustomDataset(train_input_files, train_label_files,512,384)\n","vaild_dataset = CustomDataset(vaild_input_files, vaild_label_files,512,384)\n","test_dataset = testDataset(test_all_input_files)"],"metadata":{"id":"8fU7-g5fchS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(4):\n","    origin_img = Image.open(f'/content/drive/MyDrive/train/train_input_{i+10060}.png')\n","    trans_img = train_dataset[i][0].numpy().transpose(1,2,0)\n","    fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n","    ax[0].set_title('before reduce')\n","    ax[0].imshow(origin_img)\n","    ax[1].set_title('after reduce')\n","    ax[1].imshow(trans_img)\n","    plt.show()"],"metadata":{"id":"9qtXxzricjN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset , batch_size = 4, shuffle = True)\n","vaild_loader = DataLoader(vaild_dataset , batch_size = 1, shuffle = False)\n","test_loader = DataLoader(test_dataset , batch_size = 4 , shuffle = True )"],"metadata":{"id":"pAAQ9qSUclKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 빛번짐 최소화 인공지능 모델 선정"],"metadata":{"id":"wY4ZUvbcfH-s"}},{"cell_type":"code","source":["model = smp.UnetPlusPlus(encoder_name='resnet18',\n","                encoder_weights='imagenet',\n","                in_channels=3, classes=3, activation='sigmoid')\n","\n","'''\n","model = smp.UnetPlusPlus(encoder_name='efficientnet-b4',\n","                encoder_weights='imagenet',\n","                in_channels=3, classes=3, activation='sigmoid')\n","                \n","\n","model = smp.UnetPlusPlus(encoder_name='mobilenet_v2',\n","                encoder_weights='imagenet',\n","                in_channels=3, classes=3, activation='sigmoid')\n","'''"],"metadata":{"id":"oCcj-yV9cnnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","model\n","#model.load_state_dict(torch.load('/content/drive/MyDrive/efficientnet.pt'))"],"metadata":{"id":"mAhdsAb-csQH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 빛번짐 최소화 인공지능 모델 학습"],"metadata":{"id":"3oUxgPc9fVWt"}},{"cell_type":"code","source":["criterion = torch.nn.MSELoss().to(device) #예상 답과 실제 답의 차이를 줄이기 위한 함수\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #가중치를 갱신해주는 함수\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 10, gamma=0.8)#학습에 사용 되는 가중치를 조절해주는 함수"],"metadata":{"id":"PMi40V2_cuPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss = []\n","vaild_loss=[]\n","#model.load_state_dict(torch,load('/content/drive/MyDrive/efficientnet.pt'))\n","for epoch in range(300):\n","    total_loss = 0\n","    model.train()\n","    \n","    for image , label in tqdm(train_loader,total=len(train_loader),leave = False):\n","        image = image.to(device)\n","        label = label.to(device)\n","        output = model(image.float())\n","        loss = torch.sqrt(criterion(output,label))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    train_loss.append(total_loss/len(train_loader))\n","    torch.save(model.state_dict(), '/content/drive/MyDrive/efficientnet.pt')\n","    print(\"epoch :\" , epoch+1 , \"loss :\" , total_loss/len(train_loader))\n","    \n","    scheduler.step()\n","    \n","    idx = 1\n","    '''\n","    model.eval()\n","    with torch.no_grad():\n","        for image , label in tqdm(vaild_loader,total=len(vaild_loader),leave = False):\n","            image = image.to(device)\n","            label = label.to(device)\n","            output = model(image.float())\n","            loss = torch.sqrt(criterion(output,label))\n","            \n","            total_loss += loss.item()\n","            \n","\n","            target_path = 'data/result/' + str(epoch+1)+'/'\n","            if not os.path.exists(target_path):\n","                os.makedirs(target_path, exist_ok=True)\n","\n","            grid = torchvision.utils.make_grid(tensor=torch.vstack([image,label,output]), nrow=image.shape[0])    # Saving results\n","            torchvision.utils.save_image(grid, target_path+str(idx)+'.png')\n","            idx +=1\n","        vaild_loss.append(total_loss/len(vaild_loader))\n","        print(\"val_epoch :\" , epoch+1 , \"val_loss :\" , total_loss/len(vaild_loader))\n","    '''\n","\n","#요약으로 dataset에서 문제(image)와 답지(label)을 호출 하고 model에 문제를 넣음(output = modle(image))\n","#output은 예상 답을 의미 하고 loss변수는 예상 답과 실제 답에 대한 차이를 줄여 나감\n","#zero_grad는 가중치를 초기화해주는 함수\n","#loss backward는 역전파 알고리즘에 대한 것으로 신경망 방향을 순방향으로 한번 역방향으로 한번 확인 후 비교를 하는 것\n","    "],"metadata":{"id":"qTGPK-VHc1qo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 빛번짐 모델 검증"],"metadata":{"id":"szoApC7KfdBc"}},{"cell_type":"code","source":["model.eval()\n","model.load_state_dict(torch.load( '/content/drive/MyDrive/efficientnet.pt'))\n","for image,label in vaild_loader:\n","    recon = model(image.to(device))\n","    _, ax = plt.subplots(1, 3, figsize=(15,15))\n","    ax[0].set_title('input')\n","    ax[0].imshow(np.moveaxis(image.reshape(3,384,512).cpu().numpy(),0,2))\n","    ax[1].set_title('label')\n","    ax[1].imshow(np.moveaxis(label.reshape(3,384,512).cpu().numpy(),0,2))\n","    ax[2].set_title('predict')\n","    ax[2].imshow(np.moveaxis(recon.reshape(3,384,512).cpu().detach().numpy(),0,2))"],"metadata":{"id":"pks5TKhzc4WA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 밤낮 데이터 파일 크기 및 확인"],"metadata":{"id":"qphTICP-fgkB"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCUKnN1UmzcD","executionInfo":{"status":"ok","timestamp":1653371276167,"user_tz":-540,"elapsed":17085,"user":{"displayName":"ᄋᄉᄋ","userId":"01562750966594764538"}},"outputId":"2e2ead19-8d7c-4484-d104-6b61c11d056d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["origin_img = Image.open(f'/content/drive/MyDrive/밤사진/795378f0-db2d80b2.jpg')\n","trans_img = Image.open(f'/content/drive/MyDrive/낮사진/58610009-f3b500dd.jpg')\n","fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n","ax[0].set_title('input(night)')\n","ax[0].imshow(origin_img)\n","ax[1].set_title('label(day)')\n","ax[1].imshow(trans_img)\n","plt.show()"],"metadata":{"id":"-oKoukPldAEK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 밤낮 전환 인공지능 데이터 처리"],"metadata":{"id":"BJ1I9cNCfq1A"}},{"cell_type":"code","source":["class Dataset(object):\n","    def __init__(self, label_dir, input_dir, image_size, scale):\n","        self.label_dir = [os.path.join(label_dir, x) for x in os.listdir(label_dir) if self.check_image_file(x)]\n","        self.input_dir = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if self.check_image_file(x)]\n","        self.image_size = image_size\n","        self.to_Tensor = transforms.ToTensor()\n","        self.resize = transforms.Resize((128 , 128 ), interpolation=Image.BICUBIC)\n","        self.rotates = [0, 90, 180, 270]\n","    \n","    def check_image_file(self, filename: str):\n","        return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\"])\n","    \n","    def data_augmentation(self, hr, lr):\n","\n","        width, height = hr.size\n","        \n","        hr = hr.resize((512, 384), resample=Image.BICUBIC)\n","        lr = lr.resize((512, 384), resample=Image.BICUBIC)\n","    \n","        return hr, lr\n","\n","    def __getitem__(self, idx):\n","    \n","    \n","        hr = Image.open(self.label_dir[idx]).convert(\"RGB\")\n","        lr = Image.open(self.input_dir[idx]).convert(\"RGB\") \n","\n","        hr, lr = self.data_augmentation(hr, lr) \n","        \n","        return self.to_Tensor(hr), self.to_Tensor(lr) \n","\n","    def __len__(self):\n","        return len(self.label_dir)"],"metadata":{"id":"HnPCbMGadNXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TestDataset(object):\n","    def __init__(self, input_dir, image_size, scale):\n","        self.input_dir = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if self.check_image_file(x)]\n","        self.image_size = image_size\n","        self.to_Tensor = transforms.ToTensor()\n","        self.resize = transforms.Resize((image_size , image_size ), interpolation=Image.BICUBIC)\n","        self.rotates = [0, 90, 180, 270]\n","     \n","    \n","    def check_image_file(self, filename: str):\n","        return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\"])\n","    \n","\n","    def data_augmentation(self, hr):\n","\n","        width, height = hr.size\n","\n","        hr = hr.resize((512, 384), resample=Image.BICUBIC) # 테스트용은 밤 하나이기 때문.\n","\n","        return hr\n","\n","    def __getitem__(self, idx):\n","\n","        hr = Image.open(self.input_dir[idx]).convert(\"RGB\") \n","\n","        hr = self.data_augmentation(hr) \n","        return self.to_Tensor(hr)\n","\n","    def __len__(self):\n","        return len(self.input_dir)"],"metadata":{"id":"kWWvSerEdQFC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = '/content/drive/MyDrive/밤사진/' # 밤\n","label_dir = '/content/drive/MyDrive/낮사진/' # 낮\n","test_dir = '/content/drive/MyDrive/test/' #테스트 값."],"metadata":{"id":"7IdnAkBNdR6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = Dataset(train_dir , label_dir,256,1)\n","test_dataset = TestDataset(test_dir , 256,1)"],"metadata":{"id":"hNE0MqeOdTke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pwd"],"metadata":{"id":"hBNjdbWMdU8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["origin_img = Image.open(f'/content/drive/MyDrive/밤사진/795378f0-db2d80b2.jpg')\n","trans_img = train_dataset[1][0].numpy().transpose(1,2,0)\n","fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n","ax[0].set_title('before reduce')\n","ax[0].imshow(origin_img)\n","ax[1].set_title('after reduce')\n","ax[1].imshow(trans_img)\n","plt.show()"],"metadata":{"id":"mKUkDH_odWgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["origin_img = Image.open(f'/content/drive/MyDrive/낮사진/58610009-f3b500dd.jpg')\n","trans_img = train_dataset[0][1].numpy().transpose(1,2,0)\n","fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n","ax[0].set_title('before reduce')\n","ax[0].imshow(origin_img)\n","ax[1].set_title('after reduce')\n","ax[1].imshow(trans_img)\n","plt.show()"],"metadata":{"id":"GVTOL6wIdYAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(\n","    train_dataset,\n","    batch_size = 2,\n","    shuffle = True,\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size = 3,\n","    shuffle = False\n",")"],"metadata":{"id":"TAo7_pcnddbT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# generator"],"metadata":{"id":"T5Aj0X66fxIY"}},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.ReflectionPad2d(1), # reflectionpadding이란 가장자리 기준으로 input값을 패딩영역에 반전하여 복사하여 채우는 기법. 여기선 가장자리 1줄을 채움.\n","            nn.Conv2d(in_channels, in_channels, 3), #CNN.\n","            nn.InstanceNorm2d(in_channels), #BatchNorm과 다른 점은 Batch는 전체 Dataset기준으로 Batch를 Normalize하는 것이라면, Instance는 mini-Batch단위로 instance들을 Normalize한다는 점.\n","            nn.GELU(), # dropout, zoneout, ReLu 함수의 특성을 조합한 것이 GeLU, ReLU가 뛰어나긴 하나 음수가 되어버리면 그 때의 모든 기울기는 0이 되어버리기에 음수에서 조금의 기울기를 주는 형태.\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(in_channels, in_channels, 3),\n","            nn.InstanceNorm2d(in_channels)\n","        )\n","    \n","    def forward(self, x):  # weight를 통과한 layer들과 통과하지 않은 값들을 더한 residual mapping방식. \n","        return x + self.block(x) # x는 통과하지 않은 값들, self.block(x)는 통과한 값들."],"metadata":{"id":"sdnAlycVdeJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GeneratorResNet(nn.Module):\n","    def __init__(self, in_channels, num_residual_blocks=3):  # 이 클래스에서 resiudalblock 클래스를 이용할 때 쓰이는 변수. 현재는 layer 9번 쓴다고 선언 되어있고 block수는 맘대로 선언해도 됨.\n","        super(GeneratorResNet, self).__init__()\n","        \n","        out_channels=64   #최초의 convolution 블록. 처음 출력값은 64개.\n","        self.conv = nn.Sequential(\n","            nn.ReflectionPad2d(in_channels), \n","            nn.Conv2d(in_channels, out_channels, 2*in_channels+1),# in_channels = 3 이유는 이미지는 모두 rgb를 가지고 있기 때문.\n","            nn.InstanceNorm2d(out_channels),\n","            nn.GELU(),\n","        )\n","\n","        channels = out_channels # convolution 후 나온 값을 저장한 모습. 즉, 64가 저장 된다.\n","                                \n","                                # 이 작업으로 64개의 출력값을 가지고 있는 은닉층 형성.\n","        \n","        self.down = [] #데이터 수를 줄이는게 아니라 우리 빛번짐에서 이미지 축소하고 확장하잖아? 그중 축소하면서 특징 추출하는 과정임.\n","                       #generator는 기본적으로 Unet model을 기본으로 만들기 때문에 그 중 이미지 축소하면서 특징 추출하는거지\n","                       \n","        for _ in range(2): # 2번 반복하는데 index부분이 필요가 없으므로 _ 사용.\n","            out_channels = channels * 2 #채널 값을 2배로 늘린 후 저장(첫 컨볼루션에서 3-64 여기서 64-128, 그 후 128-256)\n","            self.down += [                                                  # 이미지 축소 과정.\n","                nn.Conv2d(channels, out_channels, 3, stride=2, padding=1),\n","                nn.InstanceNorm2d(out_channels),\n","                nn.GELU(),\n","            ]\n","            channels = out_channels # 각각의 출력값을 저장. 1번째 convolution에선 128. 2번째 convolution에선 256저장.\n","        self.down = nn.Sequential(*self.down) #위에 for문 과정 down 리스트에 저장.\n","        \n","        self.trans = [ResidualBlock(channels) for _ in range(num_residual_blocks)]  #residual block을 돌리는 코드.\n","                                                                                    # for문을 풀자면 ResidualBlock클래스를 9번 돌리고 그 결괏값을 trans에 저장한다는 코드.\n","                                                                                    \n","        self.trans = nn.Sequential(*self.trans) #trans라는 리스트에 resudial block 9개층을 저장(여기도 모델에 resudial block을 몇을 주냐에 따라 층 개수 달라짐)\n","        self.up = [] # residual block을 거쳐서 mapping이 진행된 값을 저장.\n","        \n","        for _ in range(2): #upsampling과정. 여기서는 이미지를 축소했기 때문에 다시 복원하는 과정이다.\n","            out_channels = channels // 2 # 컨볼루션 과정에서 3-64-128-256형태로 축소했기 때문에 다시 256-128-64-3형태로 가야한다.\n","                                         #여기서는 이미지를 다시 확장하는 과정임\n","            self.up += [\n","                nn.Upsample(scale_factor=2), # 이미지가 만약 2*2면 scale_factor = 2면 이미지가 4*4가 됨 이렇게 확장하는과정에서 빈공간의 수를 어떻게 채울지에 따라 3가지 방식 여기선 bil...어쩌고\n","                nn.Conv2d(channels, out_channels, 3, stride=1, padding=1), \n","                nn.InstanceNorm2d(out_channels),\n","                nn.GELU(),\n","            ]\n","            channels = out_channels #위에 설명 해줬으니까 알겠지? / 다시 64를 저장.\n","        self.up = nn.Sequential(*self.up) #up list에 이미지 확장해주는 layer를 리스트로 저장\n","        \n","        self.out = nn.Sequential( #출력 layer. 이제 64-3 convolution 진행.\n","            nn.ReflectionPad2d(in_channels),\n","            nn.Conv2d(channels, in_channels, 2*in_channels+1),\n","            nn.Tanh() #마지막은 Sigmoid나 tanh둘중에 뭐쓸지 고민중 \n","        )\n","    \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.down(x)\n","        x = self.trans(x)\n","        x = self.up(x)\n","        x = self.out(x)\n","        return x"],"metadata":{"id":"w3tIzoG-dgs-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# discriminator"],"metadata":{"id":"cjG8EiZ5fz96"}},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, in_channels):\n","        super(Discriminator, self).__init__()\n","        \n","        self.model = nn.Sequential(\n","            *self.block(in_channels, 32, normalize=False),\n","            *self.block(32, 64),  \n","            *self.block(64, 128), \n","            *self.block(128, 256),\n","            \n","            nn.ZeroPad2d((1,0,1,0)), \n","            nn.Conv2d(256, 1, 4, padding=1)\n","        )\n","        \n","        self.scale_factor = 16\n","        self.m = nn.Sigmoid()\n","    \n","    @staticmethod\n","    def block(in_channels, out_channels, normalize=True):\n","        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1)]\n","        if normalize:\n","            layers.append(nn.InstanceNorm2d(out_channels))\n","        layers.append(nn.LeakyReLU(0.2, inplace=True))\n","        \n","        return layers\n","        \n","    def forward(self, x):\n","        x = self.model(x)\n","        x = self.m(x)\n","        return x\n","    "],"metadata":{"id":"rH8WwRu8dnj1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G_AB = GeneratorResNet(3, num_residual_blocks=9)\n","D_B = Discriminator(3)\n","\n","G_BA = GeneratorResNet(3, num_residual_blocks=9)\n","D_A = Discriminator(3)"],"metadata":{"id":"SZ6n-n5fdoMQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion_GAN = nn.BCELoss()\n","criterion_cycle = nn.L1Loss()\n","criterion_identity = nn.MSELoss()"],"metadata":{"id":"o8AxF5RedrVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cuda = torch.cuda.is_available()\n","print(f'cuda: {cuda}')\n","if cuda:\n","    G_AB = G_AB.cuda()\n","    #G_AB.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'G_AB2.pt'))\n","    D_B = D_B.cuda()\n","    #D_B.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'D_B2.pt'))\n","    G_BA = G_BA.cuda()\n","    #G_BA.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'G_BA2.pt'))\n","    D_A = D_A.cuda()\n","    #D_A.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'D_A2.pt'))\n","    \n","    criterion_GAN = criterion_GAN.cuda()\n","    criterion_cycle = criterion_cycle.cuda()\n","    criterion_identity = criterion_identity.cuda()"],"metadata":{"id":"3EpBbRNqdtcL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# genrator 결과 시각화"],"metadata":{"id":"SHJr568df4DE"}},{"cell_type":"code","source":["from torchvision.utils import make_grid\n","import random\n","Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n","\n","def sample_images(real_A, figside=1.5):\n","    G_AB.cuda()\n","    G_AB.eval()\n","    G_BA.eval()\n","    \n","    real_A = real_A.type(Tensor) #진짜 밤 사진.\n","    fake_B = G_AB(real_A).detach()\n","    reconv_A = G_BA(fake_B).detach()\n","    '''\n","    real_B = real_B.type(Tensor)\n","    fake_A = G_BA(real_B).detach()\n","'''\n","    \n","    nrows = real_A.size(0)\n","    real_A = make_grid(real_A, nrow=nrows, normalize=True)\n","    fake_B = make_grid(fake_B, nrow=nrows, normalize=True)\n","    reconv_A = make_grid(reconv_A, nrow=nrows, normalize=True)\n","    #fake_A = make_grid(fake_A, nrow=nrows, normalize=True)\n","    \n","    image_grid = torch.cat((real_A, fake_B, reconv_A), 1).cpu().permute(1, 2, 0)\n","    \n","    plt.figure(figsize=(15, 10))\n","    plt.imshow(image_grid)\n","    plt.axis('off')\n","    plt.show()"],"metadata":{"id":"qjKlW7iwdvMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_A = next(iter(test_loader))\n","print(real_A.shape)\n","sample_images(real_A)"],"metadata":{"id":"y_uA9xW2dxV3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 밤낮 전환 인공지능 인공지능 모델 학습"],"metadata":{"id":"Li6URnSWf7CY"}},{"cell_type":"code","source":["criterion_GAN = nn.BCELoss()\n","criterion_cycle = nn.L1Loss()\n","criterion_identity = nn.MSELoss()"],"metadata":{"id":"aWqhIxSudzKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","lr = 0.0002\n","\n","\n","optimizer_G = torch.optim.Adam(\n","    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr , betas = (0.5,0.999)\n",")\n","\n","optimizer_D_A = torch.optim.Adam(\n","    D_A.parameters(), lr=lr , betas = (0.5,0.999)\n",")\n","\n","optimizer_D_B = torch.optim.Adam(\n","    D_B.parameters(), lr=lr , betas = (0.5,0.999)\n",")"],"metadata":{"id":"0xM85BfJd1Fz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epoches = 300\n","decay_epoch = 20\n","\n","lambda_func = lambda epoch: 1 - max(0, epoch-decay_epoch)/(n_epoches-decay_epoch)\n","\n","lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_func)\n","lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambda_func)\n","lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambda_func)"],"metadata":{"id":"Y6JYlsQFd3DB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","torch.manual_seed(777)\n","\n","import gc\n","from tqdm.notebook import tqdm\n","\n","for epoch in range(100):\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    for i, (real_A, real_B) in tqdm(enumerate(train_loader),total = len(train_loader)):\n","        real_A, real_B = real_A.type(Tensor), real_B.type(Tensor)\n","        \n","        # groud truth\n","        out_shape = [real_A.size(0), 1, real_A.size(2)//D_A.scale_factor, real_A.size(3)//D_A.scale_factor]\n","        valid = torch.ones(out_shape).type(Tensor)\n","        fake = torch.zeros(out_shape).type(Tensor)\n","        \n","        G_AB.train()\n","        G_BA.train()\n","        D_A.train()\n","        D_B.train()\n","        \n","        optimizer_G.zero_grad()\n","        \n","        fake_B = G_AB(real_A) \n","        fake_A = G_BA(real_B)  \n","        \n","        loss_id_A = torch.sqrt(criterion_identity(fake_B, real_A)) \n","        loss_id_B = torch.sqrt(criterion_identity(fake_A, real_B))\n","        loss_identity = (loss_id_A + loss_id_B) / 2\n","\n","        loss_GAN_AB = criterion_GAN(D_B(fake_B),valid)\n","        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n","        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n","        \n","        recov_A = G_BA(fake_B)\n","        recov_B = G_AB(fake_A)\n","        \n","        loss_cycle_A = criterion_cycle(recov_A, real_A)\n","        loss_cycle_B = criterion_cycle(recov_B, real_B)\n","        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n","        \n","        loss_G = 5.0 *loss_identity + loss_GAN + 10.0*loss_cycle\n","        \n","        loss_G.backward()\n","        optimizer_G.step()\n","        \n","        optimizer_D_A.zero_grad()\n","        \n","        loss_real = criterion_GAN(D_A(real_A), valid)\n","        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake)\n","        loss_D_A = (loss_real + loss_fake) / 2\n","        \n","        loss_D_A.backward()\n","        optimizer_D_A.step()\n","        \n","        optimizer_D_B.zero_grad()\n","        \n","        loss_real = criterion_GAN(D_B(real_B), valid)\n","        loss_fake = criterion_GAN(D_B(fake_B.detach()), fake)\n","        loss_D_B = (loss_real + loss_fake) / 2\n","        \n","        loss_D_B.backward()\n","        optimizer_D_B.step()\n","        \n","    \n","        torch.save(G_AB.state_dict(), 'data/weight/'+f'G_AB{epoch+1}.pt') # model.load_state_dict(torch.load(G_AB2.pth))\n","        torch.save(G_BA.state_dict(), 'data/weight/'+f'G_BA{epoch+1}.pt')\n","        torch.save(D_A.state_dict(), 'data/weight/'+f'D_A{epoch+1}.pt')\n","        torch.save(D_B.state_dict(), 'data/weight/'+f'D_B{epoch+1}.pt')\n","            \n","    lr_scheduler_G.step()\n","    lr_scheduler_D_A.step()\n","    lr_scheduler_D_B.step()\n","    \n"," \n","    test_real_A = next(iter(test_loader))\n","    if (epoch+1) % 5 == 0:\n","        sample_images(test_real_A)\n","\n","    loss_D = (loss_D_A + loss_D_B) / 2\n","    if (epoch+1) % 10 == 0:\n","        print(f'[Epoch {epoch+1}/{n_epoches}]')\n","        print(f'[G loss: {loss_G.item()} | identity: {loss_identity.item()} GAN: {loss_GAN.item()} cycle: {loss_cycle.item()}]')\n","        print(f'[D loss: {loss_D.item()} | D_A: {loss_D_A.item()} D_B: {loss_D_B.item()}]') \n","        \n","        '''"],"metadata":{"id":"j6Yuv-f3d4s7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"TghPUvpHd884"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 밤낮 인공지능 검증"],"metadata":{"id":"TIrVM2wMf-oM"}},{"cell_type":"code","source":["real_A= next(iter(test_loader))\n","print(real_A.shape)\n","\n","sample_images(real_A)"],"metadata":{"id":"rxVERrXEd-lx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 전체 결과 보기(빛 줄인 후 밤낮 전환)"],"metadata":{"id":"hMSq567sgAga"}},{"cell_type":"code","source":["model.eval()\n","G_AB.eval()\n","model.load_state_dict(torch.load('/content/drive/MyDrive/efficientnet.pt'))\n","G_AB.load_state_dict(torch.load('/content/drive/MyDrive/밤.pt'))\n","with torch.no_grad():\n","    for data  in test_loader:\n","        data = data\n","        predict = model(data.to(device))  \n","        pred = G_AB(predict.to(device))\n","        #print(image.shape , output.shape ,out.shape)\n","        break"],"metadata":{"id":"d9l7aIHjeAOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(2):\n","    \n","    fix , ax = plt.subplots(ncols = 3,figsize = (20,15))\n","    ax[0].set_title('input(vaildation)')\n","    ax[0].imshow(np.moveaxis(data[i].cpu().numpy(),0,2))\n","    ax[1].set_title('reduce light')\n","    ax[1].imshow(np.moveaxis(predict[i].cpu().numpy(),0,2))\n","    ax[2].set_title('transform to day')\n","    ax[2].imshow(np.moveaxis(pred[i].cpu().numpy(),0,2))"],"metadata":{"id":"zJugMUWxeCFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 전체 결과 보기(밤낮 전환 후 빛 줄이기)"],"metadata":{"id":"_cewU4NfgENl"}},{"cell_type":"code","source":["model.eval()\n","G_AB.eval()\n","model.load_state_dict(torch.load('/content/drive/MyDrive/efficientnet.pt'))\n","G_AB.load_state_dict(torch.load('/content/drive/MyDrive/밤.pt'))\n","with torch.no_grad():\n","    for image  in test_loader:\n","        image = image\n","        output = G_AB(image.to(device))  \n","        out = model(output.to(device))\n","        print(image.shape , output.shape ,out.shape)\n","        break"],"metadata":{"id":"HUPKmSQceD0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(2):\n","    \n","    fix , ax = plt.subplots(ncols = 3,figsize = (20,15))\n","    ax[0].set_title('input(vaildation)')\n","    ax[0].imshow(np.moveaxis(image[i].cpu().numpy(),0,2))\n","    ax[1].set_title('transform to day')\n","    ax[1].imshow(np.moveaxis(output[i].cpu().numpy(),0,2))\n","    ax[2].set_title('reduce light')\n","    ax[2].imshow(np.moveaxis(out[i].cpu().numpy(),0,2))"],"metadata":{"id":"8mKzNF0NeGVK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 그냥 밤낮만 전환 했을 때랑 빛 최소화 후 밤낮 전환 했을 때랑 밤낮 전환 후 빛 최소화를 비교"],"metadata":{"id":"FACPjwPLgIhE"}},{"cell_type":"code","source":["for i in range(3):\n","    \n","    fix , ax = plt.subplots(ncols = 4,figsize = (20,15))\n","    ax[0].set_title('input(vaildation)')\n","    ax[0].imshow(np.moveaxis(image[i].cpu().numpy(),0,2))\n","    ax[1].set_title('only transform night to day')\n","    ax[1].imshow(np.moveaxis(output[i].cpu().numpy(),0,2))\n","    ax[2].set_title('reduce light and transform night to day')\n","    ax[2].imshow(np.moveaxis(pred[i].cpu().numpy(),0,2))\n","    ax[3].set_title('transform night to day and reduce light')\n","    ax[3].imshow(np.moveaxis(out[i].cpu().numpy(),0,2))"],"metadata":{"id":"kqhI6qfweH79"},"execution_count":null,"outputs":[]}]}