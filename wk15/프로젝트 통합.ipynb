{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcdd53b",
   "metadata": {},
   "source": [
    "# 필요 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm, math, random, argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from pytorch_msssim import MS_SSIM\n",
    "\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ec366",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "#device는 학습 환경을 cpu로 할 것인지 아니면 gpu로 할것인지 에 대한 변수이다.\n",
    "# torch.cuda.is_available()는 말 그대로 cuda(gpu)을 사용할 수 있으면 사용하겠다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279ba7f",
   "metadata": {},
   "source": [
    "# 빛번짐 최소화 인공지능 데이터 수집\n",
    "\n",
    "Dacon : https://dacon.io/competitions/official/235746/data\n",
    "\n",
    "DACON에서 가져온 같은장소에서 빛번짐이 존재하는 이미지 데이터 622개와 빛 번짐이 없는 이미지 데이터 622개로 이루어진 데이터셋을 제공해줘서 이 데이터셋을 다운로드. \n",
    "\n",
    " 모두 비정형 데이터 이며 이미지 크기는 3264*2448와 1632*1224 두가지의 PNG형식의 비정형 데이터임.\n",
    "\n",
    " 개발 시 622개의 데이터셋을 학습용으로 562개 검증용으로 60개로 사용하였음.\n",
    "-빛이 번져있는 사진을 인공지능에 넣었을 때 생성되는 빛이 줄이든 사진이 이미 준비해둔 빛이 안번져있는 사진과 시각 적으로 비교해서 얼마나 잘 줄어들었는지 확인하기위함.\n",
    "\n",
    " DACON에서 다운받는 데이터로만 진행했기 때문에 위의 과정을 거치고 같은 장소의 빛이 번져있는 사진과 안번져있는 사진을 구할 수 있으면 파일 형식과 사진 개수는 무관함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6bc91d",
   "metadata": {},
   "source": [
    "# directory 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c3fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:/Users/ChangHwan/Desktop/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f400f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ee6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:/Users/ChangHwan/Desktop/data/train_input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be433178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1e803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd C:/Users/ChangHwan/Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b919d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('data/train.csv')\n",
    "test_csv = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e1bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154618c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_input_files = 'data/train_input_img/'+train_csv['input_img']\n",
    "train_all_label_files = 'data/train_label_img/'+train_csv['label_img']\n",
    "test_all_input_files = 'data/test_input_img/'+test_csv['input_img']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71170a33",
   "metadata": {},
   "source": [
    "# 학습 데이터와 검증 데이터를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_files = train_all_input_files[60:].to_numpy()\n",
    "train_label_files = train_all_label_files[60:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaild_input_files = train_all_input_files[:60].to_numpy()\n",
    "vaild_label_files = train_all_label_files[:60].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92859f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9ffac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vaild_input_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4750a1",
   "metadata": {},
   "source": [
    "# 빛번짐 데이터 열어서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6c214",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    origin_img = Image.open(f'data/train_input_img/train_input_{i+10060}.png')\n",
    "    trans_img = Image.open(f'data/train_label_img/train_label_{i+10060}.png')\n",
    "    fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n",
    "    ax[0].set_title('input')\n",
    "    ax[0].imshow(origin_img)\n",
    "    ax[1].set_title('label')\n",
    "    ax[1].imshow(trans_img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b3595",
   "metadata": {},
   "source": [
    "# 빛번짐 최소화 데이터 처리\n",
    "\n",
    "제공된 데이터의 크기(3264*2448 , 1632*1224)를 512*384의 크기로 축소시키는 과정을 거침\n",
    "\n",
    " 데이터의 크기와 사용하는 인공지능 신경망을 gpu에 할당할 때 현재 colab에서 제공되는 최대 GPU는 15GB로 데이터 크기 그대로 사용했을 때 15GB로는 메모리가 부족한 현상이 발생해서 15GB에 최대한 맞는 크기로 이미지를 축소한 결과 512*384인걸 확인해서 이미지를 축소함\n",
    "\n",
    " 만약 GPU MEMORY가 더 큰 GPU를 사용한다면 최대한 이미지의 크기를 보존하도록 하는 게 좋음\n",
    "-이미지를 축소한다는 것은 이미지의 정보에 대해서 손실이 일어난다는 것을 의미하기 때문에\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, label,w,h ,is_train=True):\n",
    "        self.data  = data\n",
    "        self.label = label\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def transform(self, image, label):\n",
    "        \n",
    "        resizer = transforms.Resize(size=(self.h, self.w))\n",
    "        image = resizer(image)\n",
    "        label = resizer(label)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        label = TF.to_tensor(label)\n",
    "        return image, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        origin_img = Image.open(self.data[idx])\n",
    "        label_img  = Image.open(self.label[idx])\n",
    "        \n",
    "        origin_img, label_img = self.transform(origin_img, label_img)\n",
    "        return origin_img, label_img\n",
    "    \n",
    "#def __init__는 생성자\n",
    "#def __len__은 데이셋의 길이를 변환\n",
    "#def __transform 에서 resize = 이미지 크기 재지정 , to_tensor = 이미지를 tensor형으로 변경\n",
    "#def __getitem__은 이미지를 호출하고 transform함수를 적용 시킨 후 이미지를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7077ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, data,is_train=True):\n",
    "        self.data  = data\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def transform(self, image):\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(size=(384, 512))\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        origin_img = Image.open(self.data[idx]).convert(\"RGB\") \n",
    "        \n",
    "        origin_img = self.transform(origin_img)\n",
    "        return origin_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f279aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_input_files, train_label_files,512,384)\n",
    "vaild_dataset = CustomDataset(vaild_input_files, vaild_label_files,512,384)\n",
    "test_dataset = testDataset(test_all_input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c1496",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    origin_img = Image.open(f'data/train_input_img/train_input_{i+10060}.png')\n",
    "    trans_img = train_dataset[i][0].numpy().transpose(1,2,0)\n",
    "    fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n",
    "    ax[0].set_title('before reduce')\n",
    "    ax[0].imshow(origin_img)\n",
    "    ax[1].set_title('after reduce')\n",
    "    ax[1].imshow(trans_img)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f63723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset , batch_size = 4, shuffle = True)\n",
    "vaild_loader = DataLoader(vaild_dataset , batch_size = 1, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset , batch_size = 4 , shuffle = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a456b1",
   "metadata": {},
   "source": [
    "# 빛번짐 최소화 인공지능 모델 선정\n",
    "\n",
    " 개발 시 사용할 인공지능 신경망으로는 CNN을 선정\n",
    "다른 신경망들과 다르게 이미지 데이터를 처리 할 때 이미지의 단순한 특징 뿐만 아니라 패턴까지 알아 낼 수 있는 장점이 있기 때문임\n",
    "\n",
    " 선정한 모델로는 Resnet , Mobilenet , Efficientnet임\n",
    "-3가지 모델은 Resnet > Mobilenet > Efficinetnet으로\n",
    "인공지능의 모델의 크기에 따른 성능을 비교하기 위해 위의 모델들을 선정하였음\n",
    "\n",
    " 단 개발시에는 다른 모델을 사용해도 상관 없으나 CNN을 기초로 한 인공지능 모델을 선정하는 것을 추천\n",
    "-단순히 특징만 추출해내는 DNN같은 신경망으로 해도 문제는 없으나 효과를 기대하기는 CNN에 비해서 기대 효과를 얻기 어렵기 때문임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(encoder_name='efficientnet-b0',\n",
    "                encoder_weights='imagenet',\n",
    "                in_channels=3, classes=3, activation='sigmoid')\n",
    "\n",
    "'''\n",
    "model = smp.UnetPlusPlus(encoder_name='efficientnet-b4',\n",
    "                encoder_weights='imagenet',\n",
    "                in_channels=3, classes=3, activation='sigmoid')\n",
    "                \n",
    "\n",
    "model = smp.UnetPlusPlus(encoder_name='mobilenet_v2',\n",
    "                encoder_weights='imagenet',\n",
    "                in_channels=3, classes=3, activation='sigmoid')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07113c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model\n",
    "#model.load_state_dict(torch.load('/content/drive/MyDrive/efficientnet.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73e22d",
   "metadata": {},
   "source": [
    "# 빛번짐 최소화 인공지능 모델 학습\n",
    "\n",
    "빛번짐 최소화 인공지능 모델 학습\n",
    "\n",
    " 선정한 모델 Resnet , Mobilenet , Efficientnet을 조원끼리 하나씩 맡아서 각자의 기기로 학습을 진행\n",
    "\n",
    " 하나의 모델에 대해서 학습당 시간이 약 5분정도가 소요되고 최소한 성능을 내기 위해서는 100회 이상의 학습량이 필요했음. (ex)100회 학습 시 500분 소요)\n",
    "\n",
    " 개발 환경 구축 , 데이터 수집 , 데이터 처리까지는 각자모두 같은 과정을 수행하고 각자 모델을 하나씩 맡아서 ex)고창환은 resnet, 김선영은 mobilenet \n",
    "300번의 횟수로 학습을 진행하였음\n",
    "\n",
    "  -한 학기의 시간동안 빠르게 인공지능을 완성해야 하였기 때문에 위와 같은 방식을 선정하였음 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss().to(device) #예상 답과 실제 답의 차이를 줄이기 위한 함수\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #가중치를 갱신해주는 함수\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size = 10, gamma=0.8)#학습에 사용 되는 가중치를 조절해주는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400db9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train_loss = []\n",
    "vaild_loss=[]\n",
    "model.load_state_dict(torch.load('data/resnet.pt'))\n",
    "for epoch in range(80):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for image , label in tqdm(train_loader,total=len(train_loader),leave = False):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(image.float())\n",
    "        loss = torch.sqrt(criterion(output,label))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_loss.append(total_loss/len(train_loader))\n",
    "    torch.save(model.state_dict(), 'data/resnet.pt')\n",
    "    print(\"epoch :\" , epoch+1 , \"loss :\" , total_loss/len(train_loader))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    idx = 1\n",
    "    '''\n",
    "'''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image , label in tqdm(vaild_loader,total=len(vaild_loader),leave = False):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(image.float())\n",
    "            loss = torch.sqrt(criterion(output,label))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "\n",
    "            target_path = 'data/result/' + str(epoch+1)+'/'\n",
    "            if not os.path.exists(target_path):\n",
    "                os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "            grid = torchvision.utils.make_grid(tensor=torch.vstack([image,label,output]), nrow=image.shape[0])    # Saving results\n",
    "            torchvision.utils.save_image(grid, target_path+str(idx)+'.png')\n",
    "            idx +=1\n",
    "        vaild_loss.append(total_loss/len(vaild_loader))\n",
    "        print(\"val_epoch :\" , epoch+1 , \"val_loss :\" , total_loss/len(vaild_loader))\n",
    "    '''\n",
    "\n",
    "#요약으로 dataset에서 문제(image)와 답지(label)을 호출 하고 model에 문제를 넣음(output = modle(image))\n",
    "#output은 예상 답을 의미 하고 loss변수는 예상 답과 실제 답에 대한 차이를 줄여 나감\n",
    "#zero_grad는 가중치를 초기화해주는 함수\n",
    "#loss backward는 역전파 알고리즘에 대한 것으로 신경망 방향을 순방향으로 한번 역방향으로 한번 확인 후 비교를 하는 것\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a8d70",
   "metadata": {},
   "source": [
    "# 빛번짐 모델 검증\n",
    "\n",
    "빛번짐 최소화 인공지능 모델 검증\n",
    "\n",
    " 검증용(학습 때 사용하지 않음)으로 사용하기로 했던 60개의 빛이 번져있는 사진과 번져있지 않은 사진을 준비해서 번져있는 사진을 학습이 끝난 인공지능 모델에 넣고 그에 대한 결과를 번져있지 않은 사진과 비교해서 얼마나 빛이 잘 줄어들었는지 확인\n",
    "\n",
    " 수치적으로 이를 검증할 수 없기 때문에 시각적으로 가장 빛을 잘 줄였다고 생각한 모델을 가장 빛을 최소화시킨 모델로 선정하기로 하고 이 모델을 최종적인 빛번짐 최소화 모델로 선정\n",
    "\n",
    " resnet mobilenet efficinetnet중 efficientnet이 가장 시각적으로 빛을 잘 줄였다고 생각해 efiicinetnet을 선정하였음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629b657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load('data/efficientnet.pt'))\n",
    "for image,label in vaild_loader:\n",
    "    recon = model(image.to(device))\n",
    "    _, ax = plt.subplots(1, 3, figsize=(15,15))\n",
    "    ax[0].set_title('input')\n",
    "    ax[0].imshow(np.moveaxis(image.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "    ax[1].set_title('label')\n",
    "    ax[1].imshow(np.moveaxis(label.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "    ax[2].set_title('predict')\n",
    "    ax[2].imshow(np.moveaxis(recon.reshape(3,384,512).cpu().detach().numpy(),0,2))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be67e8a",
   "metadata": {},
   "source": [
    "# 밤낮 전환 인공지능 데이터 수집\n",
    "\n",
    "kaggle : https://www.kaggle.com/datasets/solesensei/solesensei_bdd100k\n",
    "\n",
    "Kaggle에서 낮에 찍힌 블랙박스 영상을 이미지로 분할한 사진과 밤에 블랙박스 영상을 이미지로 분할한 데이터셋을 다운로드\n",
    "\n",
    " 모두 비정형 데이터이며 이미지 크기는 모두 1080*720으로 이루어져 있고 jpg형식의 비정형 데이터임.\n",
    "\n",
    " 개발 시 다운 받은 데이터셋(약 밤사진 20000장 낮 사진 20000장)에서 밤사진 4000장 과 낮사진4000 장을 가지고 와서 학습 데이터로 사용함 \n",
    "\n",
    " 데이터를 모두 사용하면 좋으나 이 또한 한달안에 인공지능을 개발해야 하기 때문에 데이터 수를 줄여서 빠르게 결과를 보기 위해서 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb1f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd C:/Users/ChangHwan/Desktop/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9640dee",
   "metadata": {},
   "source": [
    "# 밤낮 데이터 파일 크기 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_img = Image.open(f'data/밤/795378f0-db2d80b2.jpg')\n",
    "trans_img = Image.open(f'data/낮/58610009-f3b500dd.jpg')\n",
    "fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n",
    "ax[0].set_title('input(night)')\n",
    "ax[0].imshow(origin_img)\n",
    "ax[1].set_title('label(day)')\n",
    "ax[1].imshow(trans_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:/Users/ChangHwan/Desktop/data/밤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd3c8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b9494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd C:/Users/ChangHwan/Desktop/data/낮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039ee53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef846bf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd C:/Users/ChangHwan/Desktop/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebd51e",
   "metadata": {},
   "source": [
    "# 밤낮 전환 인공지능 데이터 처리\n",
    "\n",
    "빛번짐과 똑같이 처리함 이유 또한 동일\n",
    "\n",
    "-1280 * 720 이미지를 512* 384로 축소 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, label_dir, input_dir, image_size, scale):\n",
    "        self.label_dir = [os.path.join(label_dir, x) for x in os.listdir(label_dir) if self.check_image_file(x)]\n",
    "        self.input_dir = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if self.check_image_file(x)]\n",
    "        self.image_size = image_size\n",
    "        self.to_Tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((128 , 128 ), interpolation=Image.BICUBIC)\n",
    "        self.rotates = [0, 90, 180, 270]\n",
    "    \n",
    "    def check_image_file(self, filename: str):\n",
    "        return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\"])\n",
    "    \n",
    "    def data_augmentation(self, hr, lr):\n",
    "\n",
    "        width, height = hr.size\n",
    "        \n",
    "        hr = hr.resize((512, 384), resample=Image.BICUBIC)\n",
    "        lr = lr.resize((512, 384), resample=Image.BICUBIC)\n",
    "    \n",
    "        return hr, lr\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "    \n",
    "        hr = Image.open(self.label_dir[idx]).convert(\"RGB\")\n",
    "        lr = Image.open(self.input_dir[idx]).convert(\"RGB\") \n",
    "\n",
    "        hr, lr = self.data_augmentation(hr, lr) \n",
    "        \n",
    "        return self.to_Tensor(hr), self.to_Tensor(lr) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff0bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(object):\n",
    "    def __init__(self, input_dir, image_size, scale):\n",
    "        self.input_dir = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if self.check_image_file(x)]\n",
    "        self.image_size = image_size\n",
    "        self.to_Tensor = transforms.ToTensor()\n",
    "        self.resize = transforms.Resize((image_size , image_size ), interpolation=Image.BICUBIC)\n",
    "        self.rotates = [0, 90, 180, 270]\n",
    "     \n",
    "    \n",
    "    def check_image_file(self, filename: str):\n",
    "        return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".JPG\", \".JPEG\", \".PNG\", \".BMP\"])\n",
    "    \n",
    "\n",
    "    def data_augmentation(self, hr):\n",
    "\n",
    "        width, height = hr.size\n",
    "\n",
    "        hr = hr.resize((512, 384), resample=Image.BICUBIC) # 테스트용은 밤 하나이기 때문.\n",
    "\n",
    "        return hr\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        hr = Image.open(self.input_dir[idx]).convert(\"RGB\") \n",
    "\n",
    "        hr = self.data_augmentation(hr) \n",
    "        return self.to_Tensor(hr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97529686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "train_dir = 'data/밤/' # 밤\n",
    "label_dir = 'data/낮/' # 낮\n",
    "test_dir = natsort.natsorted(glob('data/video/*.png')) #테스트 값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea65f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7da9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_dir , label_dir,256,1)\n",
    "test_dataset = testDataset(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcae294",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f179f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_img = Image.open(f'data/밤/795378f0-db2d80b2.jpg')\n",
    "trans_img = train_dataset[1][0].numpy().transpose(1,2,0)\n",
    "fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n",
    "ax[0].set_title('before reduce')\n",
    "ax[0].imshow(origin_img)\n",
    "ax[1].set_title('after reduce')\n",
    "ax[1].imshow(trans_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f4d6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "origin_img = Image.open(f'data/낮/58610009-f3b500dd.jpg')\n",
    "trans_img = train_dataset[0][1].numpy().transpose(1,2,0)\n",
    "fix , ax = plt.subplots(ncols = 2,figsize = (8,8))\n",
    "ax[0].set_title('before reduce')\n",
    "ax[0].imshow(origin_img)\n",
    "ax[1].set_title('after reduce')\n",
    "ax[1].imshow(trans_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 1,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c0b62",
   "metadata": {},
   "source": [
    "# 밤낮 전환 인공지능 모델 선정\n",
    "\n",
    " 개발 시 사용할 인공지능 신경망으로는 CNN을 선정\n",
    "다른 신경망들과 다르게 이미지 데이터를 처리 할 때 이미지의 단순한 특징 뿐만 아니라 패턴까지 알아 낼 수 있는 장점이 있기 때문임\n",
    "\n",
    " 선정한 모델로는 Cycle Gan임\n",
    "- 사용할 데이터가 낮,밤에 대해서 같은 장소에 시간이 다른 paire된 데이터가 아니라 아예 개개인의 다른 장소의 낮 , 밤으로 이루어져 있기 때문에 unpaired된 image로 데이터의 특징을 전이할 수 있는 Cycle Gan을 이용한 현재 unpaired된 데이터로 특징을 전이 할 수 있는 모델은 Cycle Gan이 유일함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc22e8",
   "metadata": {},
   "source": [
    "# generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), # reflectionpadding이란 가장자리 기준으로 input값을 패딩영역에 반전하여 복사하여 채우는 기법. 여기선 가장자리 1줄을 채움.\n",
    "            nn.Conv2d(in_channels, in_channels, 3), #CNN.\n",
    "            nn.InstanceNorm2d(in_channels), #BatchNorm과 다른 점은 Batch는 전체 Dataset기준으로 Batch를 Normalize하는 것이라면, Instance는 mini-Batch단위로 instance들을 Normalize한다는 점.\n",
    "            nn.GELU(), # dropout, zoneout, ReLu 함수의 특성을 조합한 것이 GeLU, ReLU가 뛰어나긴 하나 음수가 되어버리면 그 때의 모든 기울기는 0이 되어버리기에 음수에서 조금의 기울기를 주는 형태.\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels, 3),\n",
    "            nn.InstanceNorm2d(in_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):  # weight를 통과한 layer들과 통과하지 않은 값들을 더한 residual mapping방식. \n",
    "        return x + self.block(x) # x는 통과하지 않은 값들, self.block(x)는 통과한 값들."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_residual_blocks=3):  # 이 클래스에서 resiudalblock 클래스를 이용할 때 쓰이는 변수. 현재는 layer 9번 쓴다고 선언 되어있고 block수는 맘대로 선언해도 됨.\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        \n",
    "        out_channels=64   #최초의 convolution 블록. 처음 출력값은 64개.\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReflectionPad2d(in_channels), \n",
    "            nn.Conv2d(in_channels, out_channels, 2*in_channels+1),# in_channels = 3 이유는 이미지는 모두 rgb를 가지고 있기 때문.\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        channels = out_channels # convolution 후 나온 값을 저장한 모습. 즉, 64가 저장 된다.\n",
    "                                \n",
    "                                # 이 작업으로 64개의 출력값을 가지고 있는 은닉층 형성.\n",
    "        \n",
    "        self.down = [] #데이터 수를 줄이는게 아니라 우리 빛번짐에서 이미지 축소하고 확장하잖아? 그중 축소하면서 특징 추출하는 과정임.\n",
    "                       #generator는 기본적으로 Unet model을 기본으로 만들기 때문에 그 중 이미지 축소하면서 특징 추출하는거지\n",
    "                       \n",
    "        for _ in range(2): # 2번 반복하는데 index부분이 필요가 없으므로 _ 사용.\n",
    "            out_channels = channels * 2 #채널 값을 2배로 늘린 후 저장(첫 컨볼루션에서 3-64 여기서 64-128, 그 후 128-256)\n",
    "            self.down += [                                                  # 이미지 축소 과정.\n",
    "                nn.Conv2d(channels, out_channels, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.GELU(),\n",
    "            ]\n",
    "            channels = out_channels # 각각의 출력값을 저장. 1번째 convolution에선 128. 2번째 convolution에선 256저장.\n",
    "        self.down = nn.Sequential(*self.down) #위에 for문 과정 down 리스트에 저장.\n",
    "        \n",
    "        self.trans = [ResidualBlock(channels) for _ in range(num_residual_blocks)]  #residual block을 돌리는 코드.\n",
    "                                                                                    # for문을 풀자면 ResidualBlock클래스를 9번 돌리고 그 결괏값을 trans에 저장한다는 코드.\n",
    "                                                                                    \n",
    "        self.trans = nn.Sequential(*self.trans) #trans라는 리스트에 resudial block 9개층을 저장(여기도 모델에 resudial block을 몇을 주냐에 따라 층 개수 달라짐)\n",
    "        self.up = [] # residual block을 거쳐서 mapping이 진행된 값을 저장.\n",
    "        \n",
    "        for _ in range(2): #upsampling과정. 여기서는 이미지를 축소했기 때문에 다시 복원하는 과정이다.\n",
    "            out_channels = channels // 2 # 컨볼루션 과정에서 3-64-128-256형태로 축소했기 때문에 다시 256-128-64-3형태로 가야한다.\n",
    "                                         #여기서는 이미지를 다시 확장하는 과정임\n",
    "            self.up += [\n",
    "                nn.Upsample(scale_factor=2), # 이미지가 만약 2*2면 scale_factor = 2면 이미지가 4*4가 됨 이렇게 확장하는과정에서 빈공간의 수를 어떻게 채울지에 따라 3가지 방식 여기선 bil...어쩌고\n",
    "                nn.Conv2d(channels, out_channels, 3, stride=1, padding=1), \n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.GELU(),\n",
    "            ]\n",
    "            channels = out_channels #위에 설명 해줬으니까 알겠지? / 다시 64를 저장.\n",
    "        self.up = nn.Sequential(*self.up) #up list에 이미지 확장해주는 layer를 리스트로 저장\n",
    "        \n",
    "        self.out = nn.Sequential( #출력 layer. 이제 64-3 convolution 진행.\n",
    "            nn.ReflectionPad2d(in_channels),\n",
    "            nn.Conv2d(channels, in_channels, 2*in_channels+1),\n",
    "            nn.Tanh() #마지막은 Sigmoid나 tanh둘중에 뭐쓸지 고민중 \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.down(x)\n",
    "        x = self.trans(x)\n",
    "        x = self.up(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d029ba",
   "metadata": {},
   "source": [
    "# discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *self.block(in_channels, 32, normalize=False),\n",
    "            *self.block(32, 64),  \n",
    "            *self.block(64, 128), \n",
    "            *self.block(128, 256),\n",
    "            \n",
    "            nn.ZeroPad2d((1,0,1,0)), \n",
    "            nn.Conv2d(256, 1, 4, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.scale_factor = 16\n",
    "        self.m = nn.Sigmoid()\n",
    "    \n",
    "    @staticmethod\n",
    "    def block(in_channels, out_channels, normalize=True):\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        return layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.m(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB = GeneratorResNet(3, num_residual_blocks=6)\n",
    "D_B = Discriminator(3)\n",
    "\n",
    "G_BA = GeneratorResNet(3, num_residual_blocks=6)\n",
    "D_A = Discriminator(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d833e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(f'cuda: {cuda}')\n",
    "if cuda:\n",
    "    G_AB = G_AB.cuda()\n",
    "    #G_AB.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'G_AB2.pt'))\n",
    "    D_B = D_B.cuda()\n",
    "    #D_B.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'D_B2.pt'))\n",
    "    G_BA = G_BA.cuda()\n",
    "    #G_BA.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'G_BA2.pt'))\n",
    "    D_A = D_A.cuda()\n",
    "    #D_A.load_state_dict(torch.load('/content/drive/MyDrive/weight/'+'D_A2.pt'))\n",
    "    \n",
    "    criterion_GAN = criterion_GAN.cuda()\n",
    "    criterion_cycle = criterion_cycle.cuda()\n",
    "    criterion_identity = criterion_identity.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a15114",
   "metadata": {},
   "source": [
    "# genrator 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7189156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import random\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "def sample_images(real_A, figside=1.5):\n",
    "    G_AB.cuda()\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    real_A = real_A.type(Tensor) #진짜 밤 사진.\n",
    "    fake_B = G_AB(real_A).detach()\n",
    "    reconv_A = G_BA(fake_B).detach()\n",
    "    '''\n",
    "    real_B = real_B.type(Tensor)\n",
    "    fake_A = G_BA(real_B).detach()\n",
    "'''\n",
    "    \n",
    "    nrows = real_A.size(0)\n",
    "    real_A = make_grid(real_A, nrow=nrows, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=nrows, normalize=True)\n",
    "    reconv_A = make_grid(reconv_A, nrow=nrows, normalize=True)\n",
    "    #fake_A = make_grid(fake_A, nrow=nrows, normalize=True)\n",
    "    \n",
    "    image_grid = torch.cat((real_A, fake_B, reconv_A), 1).cpu().permute(1, 2, 0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(image_grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91614ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A = next(iter(test_loader))\n",
    "print(real_A.shape)\n",
    "sample_images(real_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c660478",
   "metadata": {},
   "source": [
    "# 밤낮 전환 인공지능 인공지능 모델 학습\n",
    "\n",
    "\n",
    "Cycle Gan의 모델 구조는 이미지를 축소 -> 이미지 특징 전이 -> 특징이 전이 된 이미지를 확장 하는 순으로 진행이 된다\n",
    "\n",
    "1번째로는 축소와 확장 횟수에 따른 결과 값을 비교하는 방식으로 진행하고 \n",
    "\n",
    "2번째로는 특징을 전이할 때 사용하는 신경망 층의 개수에 따른 성능을 비교해서\n",
    "\n",
    "가장 좋았던 축소 확장 횟수 -> 가장 좋았던 특징 전이 신경망 층 개수 순으로 최종 인공지능 모델 구조를 선정한 후 최종 학습을 진행 할 예정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86075ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lr = 0.0002\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr , betas = (0.5,0.999)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(\n",
    "    D_A.parameters(), lr=lr , betas = (0.5,0.999)\n",
    ")\n",
    "\n",
    "optimizer_D_B = torch.optim.Adam(\n",
    "    D_B.parameters(), lr=lr , betas = (0.5,0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoches = 300\n",
    "decay_epoch = 20\n",
    "\n",
    "lambda_func = lambda epoch: 1 - max(0, epoch-decay_epoch)/(n_epoches-decay_epoch)\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_func)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambda_func)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambda_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "torch.manual_seed(777)\n",
    "\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in range(100):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i, (real_A, real_B) in tqdm(enumerate(train_loader),total = len(train_loader)):\n",
    "        real_A, real_B = real_A.type(Tensor), real_B.type(Tensor)\n",
    "        \n",
    "        # groud truth\n",
    "        out_shape = [real_A.size(0), 1, real_A.size(2)//D_A.scale_factor, real_A.size(3)//D_A.scale_factor]\n",
    "        valid = torch.ones(out_shape).type(Tensor)\n",
    "        fake = torch.zeros(out_shape).type(Tensor)\n",
    "        \n",
    "        G_AB.train()\n",
    "        G_BA.train()\n",
    "        D_A.train()\n",
    "        D_B.train()\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        fake_B = G_AB(real_A) \n",
    "        fake_A = G_BA(real_B)  \n",
    "        \n",
    "        loss_id_A = torch.sqrt(criterion_identity(fake_B, real_A)) \n",
    "        loss_id_B = torch.sqrt(criterion_identity(fake_A, real_B))\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B),valid)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "        \n",
    "        recov_A = G_BA(fake_B)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        \n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "        \n",
    "        loss_G = 5.0 *loss_identity + loss_GAN + 10.0*loss_cycle\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        optimizer_D_A.zero_grad()\n",
    "        \n",
    "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake)\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "        \n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        optimizer_D_B.zero_grad()\n",
    "        \n",
    "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "        loss_fake = criterion_GAN(D_B(fake_B.detach()), fake)\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "        \n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "    \n",
    "        torch.save(G_AB.state_dict(), 'data/weight/'+f'G_AB{epoch+1}.pt') # model.load_state_dict(torch.load(G_AB2.pth))\n",
    "        torch.save(G_BA.state_dict(), 'data/weight/'+f'G_BA{epoch+1}.pt')\n",
    "        torch.save(D_A.state_dict(), 'data/weight/'+f'D_A{epoch+1}.pt')\n",
    "        torch.save(D_B.state_dict(), 'data/weight/'+f'D_B{epoch+1}.pt')\n",
    "            \n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "    \n",
    " \n",
    "    test_real_A = next(iter(test_loader))\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        sample_images(test_real_A)\n",
    "\n",
    "    loss_D = (loss_D_A + loss_D_B) / 2\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'[Epoch {epoch+1}/{n_epoches}]')\n",
    "        print(f'[G loss: {loss_G.item()} | identity: {loss_identity.item()} GAN: {loss_GAN.item()} cycle: {loss_cycle.item()}]')\n",
    "        print(f'[D loss: {loss_D.item()} | D_A: {loss_D_A.item()} D_B: {loss_D_B.item()}]') \n",
    "        \n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ced655",
   "metadata": {},
   "source": [
    "# 밤낮 인공지능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b3507",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_A= next(iter(test_loader))\n",
    "print(real_A.shape)\n",
    "\n",
    "sample_images(real_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a08fa",
   "metadata": {},
   "source": [
    "# 전체 결과 보기(빛 줄인 후 밤낮 전환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3976e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "model.eval()\n",
    "G_AB.eval()\n",
    "model.load_state_dict(torch.load('data/efficientnet.pt'))\n",
    "G_AB.load_state_dict(torch.load('data/밤.pt'))\n",
    "idx=0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        predict = model(data.to(device))  \n",
    "        pred = G_AB(predict.to(device))\n",
    "        fix , ax = plt.subplots(ncols = 3,figsize = (20,15))\n",
    "        ax[0].set_title('input(vaildation)')\n",
    "        ax[0].imshow(np.moveaxis(data.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "        ax[1].set_title('reduce light')\n",
    "        ax[1].imshow(np.moveaxis(predict.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "        ax[2].set_title('transform to day')\n",
    "        ax[2].imshow(np.moveaxis(pred.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        idx = idx+1\n",
    "        print(idx)\n",
    "\n",
    "    \n",
    "      #time.sleep(2)\n",
    "    #print(image.shape , output.shape ,out.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa54dd9b",
   "metadata": {},
   "source": [
    "# 전체 결과 보기(밤낮 전환 후 빛 줄이기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d610454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "model.eval()\n",
    "G_AB.eval()\n",
    "model.load_state_dict(torch.load('data/efficientnet.pt'))\n",
    "G_AB.load_state_dict(torch.load('data/밤.pt'))\n",
    "with torch.no_grad():\n",
    "    for image  in test_loader:\n",
    "        image = image\n",
    "        output = G_AB(image.to(device))  \n",
    "        out = model(output.to(device))\n",
    "        print(image.shape , output.shape ,out.shape)\n",
    "        fix , ax = plt.subplots(ncols = 3,figsize = (20,15))\n",
    "        ax[0].set_title('input(vaildation)')\n",
    "        ax[0].imshow(np.moveaxis(image.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "        ax[1].set_title('transform to day')\n",
    "        ax[1].imshow(np.moveaxis(output.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "        ax[2].set_title('reduce light')\n",
    "        ax[2].imshow(np.moveaxis(out.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        idx = idx+1\n",
    "        print(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071a1b1",
   "metadata": {},
   "source": [
    "# 그냥 밤낮만 전환 했을 때랑 빛 최소화 후 밤낮 전환 했을 때랑 밤낮 전환 후 빛 최소화를 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04709a50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "model.eval()\n",
    "G_AB.eval()\n",
    "model.load_state_dict(torch.load('data/efficientnet.pt'))\n",
    "G_AB.load_state_dict(torch.load('data/밤.pt'))\n",
    "with torch.no_grad():\n",
    "    idx=0\n",
    "\n",
    "    for image in test_loader:\n",
    "\n",
    "        \n",
    "        image = image\n",
    "        reduce_image = model(image.to(device))  \n",
    "        nd_image = G_AB(image.to(device))\n",
    "        predict = G_AB(reduce_image.to(device))\n",
    "        prediction = model(nd_image.to(device))\n",
    "        fix , ax = plt.subplots(ncols = 4,figsize = (30,15))\n",
    "        ax[0].set_title('input(vaildation)')\n",
    "        ax[0].imshow(np.moveaxis(image.reshape(3,384,512).cpu().numpy(),0,2))\n",
    "        ax[1].set_title('only transform night to day')\n",
    "        ax[1].imshow(np.moveaxis(nd_image.reshape(3,384,512).cpu().detach().numpy(),0,2))\n",
    "        ax[2].set_title('reduce light and transform night to day')\n",
    "        ax[2].imshow(np.moveaxis(predict.reshape(3,384,512).cpu().detach().numpy(),0,2))\n",
    "        ax[3].set_title('transform night to day and reduce light')\n",
    "        ax[3].imshow(np.moveaxis(prediction.reshape(3,384,512).cpu().detach().numpy(),0,2))\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        idx = idx+1\n",
    "        print(idx)\n",
    "        #plt.show()\n",
    "        #time.sleep(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef59bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
